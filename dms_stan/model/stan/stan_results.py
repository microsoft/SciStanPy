import os.path

import arviz as az
import cmdstanpy
import numpy as np
import numpy.typing as npt

import dms_stan.model.stan as stan_module

from dms_stan.model.components import Normal


class SampleResults:
    def __init__(
        self,
        stan_model: "stan_module.StanModel",
        fit: cmdstanpy.CmdStanMCMC,
        data: dict[str, npt.NDArray],
        _from_disk: bool = False,
    ):
        # If loading from disk, skip the rest of the initialization
        if _from_disk:
            return

        # Store the CmdStanMCMC object and the observed data
        self.stan_model = stan_model
        self.fit = fit

        # Get the additional arguments needed for building the arviz object
        posterior_predictive = self._get_ppc(data)
        coords, dims = self._get_coords_dims()

        # Build the arviz object
        self.inference_obj = az.from_cmdstanpy(
            posterior=fit,
            posterior_predictive=posterior_predictive,
            observed_data=data,
            constant_data=self.stan_model.autogathered_data,
            coords=coords,
            dims=dims,
        )

    def _get_ppc(self, data: dict[str, npt.NDArray]) -> list[str]:

        # Note the difference between the provided observed data and the known
        # observed data
        expected_observations = {
            name for name in self.fit.stan_variables().keys() if name.endswith("_ppc")
        }
        actual_observations = {k + "_ppc" for k in data.keys()}
        if additional_observations := actual_observations - expected_observations:
            raise ValueError(
                "The following observations were provided as data, but there were "
                "no samples generated for them by the Stan model: "
                + ", ".join(additional_observations)
            )
        if missing_observations := expected_observations - actual_observations:
            raise ValueError(
                "The following observations were expected to be provided as data, "
                "but were not: " + ", ".join(missing_observations)
            )

        return list(expected_observations)

    def _get_coords_dims(
        self,
    ) -> tuple[dict[str, npt.NDArray[np.int64]], dict[str, list[str]]]:
        """Get the coordinates and dimensions for the ArviZ object"""
        # Set up variables for recording
        varname_to_named_shape: dict[str, list[str]] = {}  # Named shapes
        dummies: set[str] = set()  # Dummy dimension names for singletons

        # Get a map from dimension depth and size to dimension name
        dim_map = self.stan_model.model.get_dimname_map()

        # We also need the variables for which there are samples generated by Stan
        sampled_varnames = set(self.fit.stan_variables().keys())

        # Process all variables
        for varname in self.stan_model.program.all_varnames:

            # Get the stan-friendly variable name
            stan_varname = varname.replace(".", "__")

            # Get the model component
            model_component = self.stan_model.model[varname]

            # Get the name of the dimensions
            named_shape = [None] * model_component.ndim
            for dimind, dimsize in enumerate(model_component.shape[::-1]):

                # See if we can get the name of the dimension. If we cannot, this must
                # be a singleton dimension
                if (dimname := dim_map.get((dimind, dimsize))) is None:
                    assert dimsize == 1
                    dimname = f"dummy_{dimind}"
                named_shape[dimind] = dimname

            # Scalars are a special case unless they are sampled
            if model_component.ndim == 0 and stan_varname not in sampled_varnames:
                named_shape = ["dummy_0"]

            # Update the set of dummies
            dummies.update(name for name in named_shape if name.startswith("dummy_"))

            # Update the mapping
            named_shape = named_shape[::-1]
            varname_to_named_shape[stan_varname] = named_shape

            # If an observable, also add the posterior predictive samples
            if model_component.observable:
                varname_to_named_shape[f"{stan_varname}_ppc"] = named_shape

            # If non-centered, also add the "raw" version
            if (
                isinstance(model_component, Normal)
                and not model_component.is_hyperparameter
            ):
                varname_to_named_shape[f"{stan_varname}_raw"] = named_shape

        # Get the coordinates
        coords: dict[str, npt.NDArray[np.int64]] = {
            name: np.arange(dimsize) for (_, dimsize), name in dim_map.items()
        } | {dummy: np.array([0]) for dummy in dummies}

        return coords, varname_to_named_shape

    def _save_netcdf(self) -> None:
        """
        Saves the ArViz object to a netcdf file. This is performed on initialization
        when not loading from disk
        """
        # Get the common prefix for the csv files attached to the CmdStanMCMC object
        prefix = os.path.commonprefix(self.fit.runset.csv_files)

        # The directory name of the prefix should be the output directory of the
        # DMSStanModel
        assert os.path.dirname(prefix) == self.stan_model.output_dir

        # Save the ArViZ object to a netcdf file
        self.inference_obj.to_netcdf(prefix + "arviz.nc")

    # Missing attributes are pulled from the CmdStanMCMC object
    def __getattr__(self, name):
        return getattr(self.fit, name)

    @classmethod
    def from_disk(cls, prefix: str) -> "SampleResults":
        """
        Loads the object from disk. The prefix is the common prefix of the csv and
        netcdf file output during sampling.
        """
