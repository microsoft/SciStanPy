<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>scistanpy.model.nn_module &#8212; SciStanPy Alpha documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../../../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css?v=27fed22d" />
    <script src="../../../_static/documentation_options.js?v=6b921976"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for scistanpy.model.nn_module</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Microsoft Corporation.</span>
<span class="c1"># Licensed under the MIT license.</span>


<span class="sd">&quot;&quot;&quot;PyTorch integration utilities for SciStanPy models.</span>

<span class="sd">This module provides integration between SciStanPy probabilistic models</span>
<span class="sd">and PyTorch&#39;s automatic differentiation and optimization framework. It enables</span>
<span class="sd">maximum likelihood estimation, variational inference, and other gradient-based</span>
<span class="sd">learning procedures on SciStanPy models.</span>

<span class="sd">The module&#39;s core functionality centers around converting SciStanPy models into</span>
<span class="sd">PyTorch ``nn.Module`` instances that preserve the probabilistic structure while</span>
<span class="sd">enabling efficient gradient computation and optimization. This allows users to</span>
<span class="sd">leverage PyTorch&#39;s ecosystem of optimizers, learning rate schedulers, and other</span>
<span class="sd">training utilities.</span>

<span class="sd">Key Features:</span>
<span class="sd">    - Automatic conversion of SciStanPy models to PyTorch modules</span>
<span class="sd">    - Gradient-based parameter optimization with various optimizers</span>
<span class="sd">    - Mixed precision training support for improved performance</span>
<span class="sd">    - Early stopping and convergence monitoring</span>
<span class="sd">    - GPU acceleration and device management</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">itertools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy.typing</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">npt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">scistanpy.defaults</span><span class="w"> </span><span class="kn">import</span> <span class="n">DEFAULT_EARLY_STOP</span><span class="p">,</span> <span class="n">DEFAULT_LR</span><span class="p">,</span> <span class="n">DEFAULT_N_EPOCHS</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scistanpy.model.components</span><span class="w"> </span><span class="kn">import</span> <span class="n">constants</span><span class="p">,</span> <span class="n">parameters</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">scistanpy</span><span class="w"> </span><span class="kn">import</span> <span class="n">custom_types</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">scistanpy</span><span class="w"> </span><span class="kn">import</span> <span class="n">model</span> <span class="k">as</span> <span class="n">ssp_model</span>


<span class="k">def</span><span class="w"> </span><span class="nf">check_observable_data</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="s2">&quot;ssp_model.Model&quot;</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Validate that provided data matches model observable specifications.</span>

<span class="sd">    This function performs comprehensive validation to ensure that the observed</span>
<span class="sd">    data dictionary contains exactly the expected observables with correct</span>
<span class="sd">    shapes and types. It prevents common errors during model fitting by</span>
<span class="sd">    catching data mismatches early.</span>

<span class="sd">    :param model: SciStanPy model containing observable specifications</span>
<span class="sd">    :type model: ssp_model.Model</span>
<span class="sd">    :param data: Dictionary mapping observable names to their tensor data</span>
<span class="sd">    :type data: dict[str, torch.Tensor]</span>

<span class="sd">    :raises ValueError: If observable names don&#39;t match expected set</span>
<span class="sd">    :raises ValueError: If data shapes don&#39;t match observable shapes</span>

<span class="sd">    The validation checks:</span>
<span class="sd">        - Perfect correspondence between provided and expected observable names</span>
<span class="sd">        - Exact shape matching between data tensors and observable specifications</span>
<span class="sd">        - Proper tensor formatting for PyTorch computation</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; data = {&#39;y&#39;: torch.randn(100), &#39;x&#39;: torch.randn(100, 5)}</span>
<span class="sd">        &gt;&gt;&gt; check_observable_data(model, data)  # Validates or raises error</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># There must be perfect overlap between the keys of the provided data and the</span>
    <span class="c1"># expected observations</span>
    <span class="n">expected_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">observable_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">provided_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">missing</span> <span class="o">=</span> <span class="n">expected_set</span> <span class="o">-</span> <span class="n">provided_set</span>
    <span class="n">extra</span> <span class="o">=</span> <span class="n">provided_set</span> <span class="o">-</span> <span class="n">expected_set</span>

    <span class="c1"># If there are missing or extra, raise an error</span>
    <span class="k">if</span> <span class="n">missing</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;The provided data must match the observable distribution names.&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;The following observables are missing: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">missing</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">extra</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;The provided data must match the observable distribution names. The &quot;</span>
            <span class="s2">&quot;following observables were provided in addition to the expected: &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">extra</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Shapes must match</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">observable_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The shape of the provided data for observable </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> does not match &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;the expected shape. Expected: </span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, provided: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">data</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>


<div class="viewcode-block" id="PyTorchModel">
<a class="viewcode-back" href="../../../api/model/nn_module.html#scistanpy.model.nn_module.PyTorchModel">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">PyTorchModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;PyTorch-trainable version of a SciStanPy Model.</span>

<span class="sd">    This class converts SciStanPy probabilistic models into PyTorch nn.Module</span>
<span class="sd">    instances that can be optimized using standard PyTorch training procedures.</span>
<span class="sd">    It preserves the probabilistic structure while enabling gradient-based</span>
<span class="sd">    parameter estimation and other machine learning techniques.</span>

<span class="sd">    :param model: SciStanPy model to convert to PyTorch</span>
<span class="sd">    :type model: ssp_model.Model</span>
<span class="sd">    :param seed: Random seed for reproducible parameter initialization. Defaults to None.</span>
<span class="sd">    :type seed: Optional[custom_types.Integer]</span>

<span class="sd">    :ivar model: Reference to the original SciStanPy model</span>
<span class="sd">    :ivar learnable_params: PyTorch ParameterList containing optimizable parameters</span>

<span class="sd">    The conversion process:</span>
<span class="sd">        - Initializes all model parameters for PyTorch optimization</span>
<span class="sd">        - Sets up proper gradient computation graphs</span>
<span class="sd">        - Configures device placement and memory management</span>
<span class="sd">        - Preserves probabilistic model structure and relationships</span>

<span class="sd">    The resulting PyTorch model can be treated like any other nn.Module.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; pytorch_model = model.to_pytorch(seed=42)</span>
<span class="sd">        &gt;&gt;&gt; optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=0.01)</span>
<span class="sd">        &gt;&gt;&gt; loss = -pytorch_model(**observed_data)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">        &gt;&gt;&gt; optimizer.step()</span>

<span class="sd">    .. note::</span>
<span class="sd">        This class should not be instantiated directly. Instead, use the</span>
<span class="sd">        `to_pytorch()` method on a SciStanPy Model instance.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="s2">&quot;ssp_model.Model&quot;</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;custom_types.Integer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the PyTorch model from a SciStanPy model.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Record the model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>

        <span class="c1"># Initialize all parameters for pytorch optimization</span>
        <span class="n">learnable_params</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">param_num</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">):</span>
            <span class="n">param</span><span class="o">.</span><span class="n">init_pytorch</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span> <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">seed</span> <span class="o">+</span> <span class="n">param_num</span><span class="p">)</span>
            <span class="n">learnable_params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">_torch_parametrization</span><span class="p">)</span>

        <span class="c1"># Record learnable parameters such that they can be recognized by PyTorch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learnable_params</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterList</span><span class="p">(</span><span class="n">learnable_params</span><span class="p">)</span>

<div class="viewcode-block" id="PyTorchModel.forward">
<a class="viewcode-back" href="../../../api/model/nn_module.html#scistanpy.model.nn_module.PyTorchModel.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute log probability of observed data given current parameters.</span>

<span class="sd">        This method calculates the total log probability (log-likelihood) of</span>
<span class="sd">        the observed data under the current model parameters. It forms the</span>
<span class="sd">        core objective function for maximum likelihood estimation and other</span>
<span class="sd">        gradient-based inference procedures.</span>

<span class="sd">        :param data: Observed data tensors keyed by observable parameter names</span>
<span class="sd">        :type data: dict[str, torch.Tensor]</span>

<span class="sd">        :returns: Total log probability of the observed data</span>
<span class="sd">        :rtype: torch.Tensor</span>

<span class="sd">        .. important::</span>
<span class="sd">            This returns log probability, *not* log loss (negative log probability).</span>
<span class="sd">            For optimization, negate the result to get the loss function.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; log_prob = pytorch_model(y=observed_y, x=observed_x)</span>
<span class="sd">            &gt;&gt;&gt; loss = -log_prob  # Negative for minimization</span>
<span class="sd">            &gt;&gt;&gt; loss.backward()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Sum the log-probs of the observables and parameters</span>
        <span class="n">log_prob</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameter_dict</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">observable_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="c1"># Calculate the log probability of the observed data given the parameters</span>
            <span class="n">temp_log_prob</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">get_torch_logprob</span><span class="p">(</span><span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>

            <span class="c1"># Log probability should be 0-dimensional if anything but a Multinomial</span>
            <span class="k">assert</span> <span class="n">temp_log_prob</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">parameters</span><span class="o">.</span><span class="n">Multinomial</span><span class="p">)</span>

            <span class="c1"># Add to the total log probability</span>
            <span class="n">log_prob</span> <span class="o">+=</span> <span class="n">temp_log_prob</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">log_prob</span></div>


<div class="viewcode-block" id="PyTorchModel.fit">
<a class="viewcode-back" href="../../../api/model/nn_module.html#scistanpy.model.nn_module.PyTorchModel.fit">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">epochs</span><span class="p">:</span> <span class="s2">&quot;custom_types.Integer&quot;</span> <span class="o">=</span> <span class="n">DEFAULT_N_EPOCHS</span><span class="p">,</span>
        <span class="n">early_stop</span><span class="p">:</span> <span class="s2">&quot;custom_types.Integer&quot;</span> <span class="o">=</span> <span class="n">DEFAULT_EARLY_STOP</span><span class="p">,</span>
        <span class="n">lr</span><span class="p">:</span> <span class="s2">&quot;custom_types.Float&quot;</span> <span class="o">=</span> <span class="n">DEFAULT_LR</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span>
            <span class="nb">str</span><span class="p">,</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">,</span> <span class="s2">&quot;custom_types.Float&quot;</span><span class="p">,</span> <span class="s2">&quot;custom_types.Integer&quot;</span>
            <span class="p">],</span>
        <span class="p">],</span>
        <span class="n">mixed_precision</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Optimize model parameters using gradient-based maximum likelihood estimation.</span>

<span class="sd">        This method performs complete model training using the Adam optimizer</span>
<span class="sd">        with configurable early stopping, learning rate, and mixed precision</span>
<span class="sd">        support. It automatically handles device placement, gradient computation,</span>
<span class="sd">        and convergence monitoring.</span>

<span class="sd">        :param epochs: Maximum number of training epochs. Defaults to 100000.</span>
<span class="sd">        :type epochs: custom_types.Integer</span>
<span class="sd">        :param early_stop: Epochs without improvement before stopping. Defaults to 10.</span>
<span class="sd">        :type early_stop: custom_types.Integer</span>
<span class="sd">        :param lr: Learning rate for Adam optimizer. Defaults to 0.001.</span>
<span class="sd">        :type lr: custom_types.Float</span>
<span class="sd">        :param data: Observed data for model observables</span>
<span class="sd">        :type data: dict[str, Union[torch.Tensor, npt.NDArray, custom_types.Float,</span>
<span class="sd">            custom_types.Integer]]</span>
<span class="sd">        :param mixed_precision: Whether to use automatic mixed precision. Defaults to False.</span>
<span class="sd">        :type mixed_precision: bool</span>

<span class="sd">        :returns: Tensor containing loss trajectory throughout training</span>
<span class="sd">        :rtype: torch.Tensor</span>

<span class="sd">        :raises UserWarning: If early stopping is not triggered within epoch limit</span>

<span class="sd">        The training loop:</span>
<span class="sd">            1. Converts input data to appropriate tensor format</span>
<span class="sd">            2. Validates data compatibility with model observables</span>
<span class="sd">            3. Iteratively optimizes parameters using gradient descent</span>
<span class="sd">            4. Monitors convergence and applies early stopping</span>
<span class="sd">            5. Returns complete loss trajectory for analysis</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; loss_history = pytorch_model.fit(</span>
<span class="sd">            ...     data={&#39;y&#39;: observed_data},</span>
<span class="sd">            ...     epochs=5000,</span>
<span class="sd">            ...     lr=0.01,</span>
<span class="sd">            ...     early_stop=50,</span>
<span class="sd">            ...     mixed_precision=True</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; final_loss = loss_history[-1]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Any observed data that is not a tensor is converted to a tensor</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">v</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>

        <span class="c1"># Note the device</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learnable_params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span>

        <span class="c1"># Check the observed data</span>
        <span class="n">check_observable_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

        <span class="c1"># Train mode. This should be a null-op.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

        <span class="c1"># Build the optimizer</span>
        <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

        <span class="c1"># If using mixed precision, we also need a scaler</span>
        <span class="k">if</span> <span class="n">mixed_precision</span><span class="p">:</span>
            <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">GradScaler</span><span class="p">()</span>

        <span class="c1"># Set up for optimization</span>
        <span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>  <span class="c1"># Records the best loss</span>
        <span class="n">loss_trajectory</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Records all losses</span>
        <span class="n">n_without_improvement</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Epochs without improvement</span>

        <span class="c1"># Run optimization</span>
        <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Epochs&quot;</span><span class="p">,</span> <span class="n">postfix</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;-log pdf/pmf&quot;</span><span class="p">:</span> <span class="s2">&quot;N/A&quot;</span><span class="p">})</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

                <span class="c1"># Get the loss</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="n">mixed_precision</span><span class="p">):</span>
                    <span class="n">log_loss</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">data</span><span class="p">)</span>

                <span class="c1"># Step the optimizer</span>
                <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">mixed_precision</span><span class="p">:</span>
                    <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">log_loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                    <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optim</span><span class="p">)</span>
                    <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">log_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                    <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

                <span class="c1"># Record loss</span>
                <span class="n">log_loss</span> <span class="o">=</span> <span class="n">log_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">loss_trajectory</span><span class="p">[</span><span class="n">epoch</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_loss</span>

                <span class="c1"># Update best loss</span>
                <span class="k">if</span> <span class="n">log_loss</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
                    <span class="n">n_without_improvement</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">best_loss</span> <span class="o">=</span> <span class="n">log_loss</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">n_without_improvement</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># Update progress bar</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;-log pdf/pmf&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">log_loss</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">})</span>

                <span class="c1"># Check for early stopping</span>
                <span class="k">if</span> <span class="n">early_stop</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n_without_improvement</span> <span class="o">&gt;=</span> <span class="n">early_stop</span><span class="p">:</span>
                    <span class="k">break</span>

            <span class="c1"># Note that early stopping was not triggered if the loop completes</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">early_stop</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Early stopping not triggered.&quot;</span><span class="p">)</span>

        <span class="c1"># Back to eval mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="c1"># Get a final loss</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">loss_trajectory</span><span class="p">[</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Trim off the None values of the loss trajectory and convert to a tensor</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">loss_trajectory</span><span class="p">[:</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span></div>


<div class="viewcode-block" id="PyTorchModel.export_params">
<a class="viewcode-back" href="../../../api/model/nn_module.html#scistanpy.model.nn_module.PyTorchModel.export_params">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">export_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Export optimized parameter values from the fitted model.</span>

<span class="sd">        This method extracts the current parameter values after optimization,</span>
<span class="sd">        providing access to the maximum likelihood estimates or other fitted</span>
<span class="sd">        parameter values. It excludes observable parameters (which represent</span>
<span class="sd">        data) and focuses on the learnable model parameters.</span>

<span class="sd">        :returns: Dictionary mapping parameter names to their current tensor values</span>
<span class="sd">        :rtype: dict[str, torch.Tensor]</span>

<span class="sd">        Excluded from export:</span>
<span class="sd">            - Observable parameters (representing data, not learnable parameters)</span>
<span class="sd">            - Unnamed parameters</span>
<span class="sd">            - Intermediate computational results from transformations</span>

<span class="sd">        This is typically used after model fitting to extract the estimated</span>
<span class="sd">        parameter values for further analysis or model comparison.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; fitted_params = pytorch_model.export_params()</span>
<span class="sd">            &gt;&gt;&gt; mu_estimate = fitted_params[&#39;mu&#39;]</span>
<span class="sd">            &gt;&gt;&gt; sigma_estimate = fitted_params[&#39;sigma&#39;]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="n">name</span><span class="p">:</span> <span class="n">param</span><span class="o">.</span><span class="n">torch_parametrization</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameter_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span></div>


<div class="viewcode-block" id="PyTorchModel.export_distributions">
<a class="viewcode-back" href="../../../api/model/nn_module.html#scistanpy.model.nn_module.PyTorchModel.export_distributions">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">export_distributions</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Export fitted probability distributions for all model components.</span>

<span class="sd">        This method returns the complete set of probability distributions</span>
<span class="sd">        from the fitted model, including both parameter distributions (priors)</span>
<span class="sd">        and observable distributions (likelihoods) with their current</span>
<span class="sd">        parameter values.</span>

<span class="sd">        :returns: Dictionary mapping component names to their distribution objects</span>
<span class="sd">        :rtype: dict[str, torch.distributions.Distribution]</span>

<span class="sd">        The exported distributions include:</span>
<span class="sd">            - Parameter distributions with updated hyperparameter values</span>
<span class="sd">            - Observable distributions with fitted parameter values</span>
<span class="sd">            - All distributions in their PyTorch format for further computation</span>

<span class="sd">        This is useful for:</span>
<span class="sd">            - Posterior predictive sampling</span>
<span class="sd">            - Model diagnostics and validation</span>
<span class="sd">            - Uncertainty quantification</span>
<span class="sd">            - Distribution comparison and analysis</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; distributions = pytorch_model.export_distributions()</span>
<span class="sd">            &gt;&gt;&gt; fitted_normal = distributions[&#39;mu&#39;]  # torch.distributions.Normal</span>
<span class="sd">            &gt;&gt;&gt; samples = fitted_normal.sample((1000,))  # Sample from fit distribution</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="n">name</span><span class="p">:</span> <span class="n">param</span><span class="o">.</span><span class="n">torch_dist_instance</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameter_dict</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">observable_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="p">}</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_move_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">funcname</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Internal method for device placement operations.</span>

<span class="sd">        This method handles the task of moving both PyTorch parameters</span>
<span class="sd">        and SciStanPy constant tensors to different devices or data types.</span>
<span class="sd">        It ensures that all model components remain synchronized during</span>
<span class="sd">        device transfers.</span>

<span class="sd">        :param funcname: Name of the PyTorch method to apply (&#39;cuda&#39;, &#39;cpu&#39;, &#39;to&#39;)</span>
<span class="sd">        :type funcname: str</span>
<span class="sd">        :param args: Positional arguments for the device operation</span>
<span class="sd">        :param kwargs: Keyword arguments for the device operation</span>

<span class="sd">        :returns: Self reference for method chaining</span>
<span class="sd">        :rtype: PyTorchModel</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Apply to the model</span>
        <span class="nb">getattr</span><span class="p">(</span><span class="nb">super</span><span class="p">(),</span> <span class="n">funcname</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Apply to additional torch tensors in the model (i.e., the ones that are</span>
        <span class="c1"># constants and not parameters)</span>
        <span class="c1"># pylint: disable=protected-access</span>
        <span class="k">for</span> <span class="n">constant</span> <span class="ow">in</span> <span class="nb">filter</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">constants</span><span class="o">.</span><span class="n">Constant</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">all_model_components</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">constant</span><span class="o">.</span><span class="n">_torch_parametrization</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span>
                <span class="n">constant</span><span class="o">.</span><span class="n">_torch_parametrization</span><span class="p">,</span> <span class="n">funcname</span>
            <span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

<div class="viewcode-block" id="PyTorchModel.cuda">
<a class="viewcode-back" href="../../../api/model/nn_module.html#scistanpy.model.nn_module.PyTorchModel.cuda">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Move model to CUDA device.</span>

<span class="sd">        This method transfers the entire model (including SciStanPy constants)</span>
<span class="sd">        to a CUDA-enabled GPU device for accelerated computation.</span>

<span class="sd">        :param args: Arguments passed to torch.nn.Module.cuda()</span>
<span class="sd">        :param kwargs: Keyword arguments passed to torch.nn.Module.cuda()</span>

<span class="sd">        :returns: Self reference for method chaining</span>
<span class="sd">        :rtype: PyTorchModel</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; pytorch_model = pytorch_model.cuda()  # Move to default GPU</span>
<span class="sd">            &gt;&gt;&gt; pytorch_model = pytorch_model.cuda(1)  # Move to GPU 1</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_move_model</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="PyTorchModel.cpu">
<a class="viewcode-back" href="../../../api/model/nn_module.html#scistanpy.model.nn_module.PyTorchModel.cpu">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Move model to CPU device.</span>

<span class="sd">        This method transfers the entire model (including SciStanPy constants)</span>
<span class="sd">        to CPU memory, which is useful for inference or when GPU memory is limited.</span>

<span class="sd">        :param args: Arguments passed to torch.nn.Module.cpu()</span>
<span class="sd">        :param kwargs: Keyword arguments passed to torch.nn.Module.cpu()</span>

<span class="sd">        :returns: Self reference for method chaining</span>
<span class="sd">        :rtype: PyTorchModel</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; pytorch_model = pytorch_model.cpu()  # Move to CPU</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_move_model</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="PyTorchModel.to">
<a class="viewcode-back" href="../../../api/model/nn_module.html#scistanpy.model.nn_module.PyTorchModel.to">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Move model to specified device or data type.</span>

<span class="sd">        This method provides flexible device and dtype conversion for the</span>
<span class="sd">        entire model, including both PyTorch parameters and SciStanPy</span>
<span class="sd">        constant tensors.</span>

<span class="sd">        :param args: Arguments passed to torch.nn.Module.to()</span>
<span class="sd">        :param kwargs: Keyword arguments passed to torch.nn.Module.to()</span>

<span class="sd">        :returns: Self reference for method chaining</span>
<span class="sd">        :rtype: PyTorchModel</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; pytorch_model = pytorch_model.to(&#39;cuda:0&#39;)  # Move to specific GPU</span>
<span class="sd">            &gt;&gt;&gt; pytorch_model = pytorch_model.to(torch.float64)  # Change precision</span>
<span class="sd">            &gt;&gt;&gt; pytorch_model = pytorch_model.to(&#39;cpu&#39;, dtype=torch.float32)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_move_model</span><span class="p">(</span><span class="s2">&quot;to&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>
</div>

</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">SciStanPy</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/index.html">SciStanPy API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/index.html">Examples</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  <li><a href="../../scistanpy.html">scistanpy</a><ul>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Microsoft Corporation.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.3.0</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
    </div>

    

    
  </body>
</html>