<!DOCTYPE html>

<html lang="en" data-content_root="../../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Custom PyTorch Distributions API Reference &#8212; SciStanPy Alpha documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css?v=27fed22d" />
    <script src="../../../../_static/documentation_options.js?v=6b921976"></script>
    <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Custom SciPy Distributions API Reference" href="custom_scipy_dists.html" />
    <link rel="prev" title="Custom Distributions API Reference" href="index.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="custom-pytorch-distributions-api-reference">
<h1>Custom PyTorch Distributions API Reference<a class="headerlink" href="#custom-pytorch-distributions-api-reference" title="Link to this heading">¶</a></h1>
<p>This reference covers the custom PyTorch-based probability distributions in SciStanPy.</p>
<section id="module-scistanpy.model.components.custom_distributions.custom_torch_dists">
<span id="custom-torch-distributions-module"></span><h2>Custom Torch Distributions Module<a class="headerlink" href="#module-scistanpy.model.components.custom_distributions.custom_torch_dists" title="Link to this heading">¶</a></h2>
<p>Custom PyTorch distribution implementations for SciStanPy models.</p>
<p>This module provides specialized PyTorch distribution classes that extend or
modify the standard PyTorch distributions to meet specific requirements of
SciStanPy modeling. These distributions handle edge cases, provide numerical
stability improvements, and enable functionality not available in the standard
PyTorch distribution library.</p>
<dl class="simple">
<dt>Key Features:</dt><dd><ul class="simple">
<li><p><strong>Extended Multinomial</strong>: Support for inhomogeneous total counts</p></li>
<li><p><strong>Numerical Stability</strong>: Improved log-space probability computations</p></li>
<li><p><strong>Custom Distributions</strong>: Implementations of distributions not in PyTorch</p></li>
<li><p><strong>SciStanPy Integration</strong>: Designed for compatibility with SciStanPy parameter types</p></li>
</ul>
</dd>
</dl>
<p>Distribution Categories:</p>
<dl class="simple">
<dt><strong>Multinomial Extensions</strong>: Enhanced multinomial distributions</dt><dd><ul class="simple">
<li><p>Multinomial: Base class with inhomogeneous total count support</p></li>
<li><p>MultinomialProb: Probability-parameterized multinomial</p></li>
<li><p>MultinomialLogit: Logit-parameterized multinomial</p></li>
<li><p>MultinomialLogTheta: Normalized log-probability multinomial</p></li>
</ul>
</dd>
<dt><strong>Numerically Stable Distributions</strong>: Improved standard distributions</dt><dd><ul class="simple">
<li><p>Normal: Enhanced with stable log-CDF and log-survival functions</p></li>
<li><p>LogNormal: Enhanced with stable log-space probability functions</p></li>
</ul>
</dd>
<dt><strong>Custom Distribution Implementations</strong>: New distribution types</dt><dd><ul class="simple">
<li><p>Lomax: Shifted Pareto distribution</p></li>
<li><p>ExpLomax: Exponential-Lomax distribution</p></li>
<li><p>ExpExponential: Exponential-Exponential distribution</p></li>
<li><p>ExpDirichlet: Exponential-Dirichlet distribution</p></li>
</ul>
</dd>
</dl>
<p>The distributions in this module are designed to work within PyTorch’s
distribution framework while providing the specific functionality required
for probabilistic modeling in SciStanPy.</p>
</section>
<section id="distribution-architecture">
<h2>Distribution Architecture<a class="headerlink" href="#distribution-architecture" title="Link to this heading">¶</a></h2>
<section id="base-classes">
<h3>Base Classes<a class="headerlink" href="#base-classes" title="Link to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scistanpy.model.components.custom_distributions.custom_torch_dists.CustomDistribution">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scistanpy.model.components.custom_distributions.custom_torch_dists.</span></span><span class="sig-name descname"><span class="pre">CustomDistribution</span></span><a class="reference internal" href="../../../../_modules/scistanpy/model/components/custom_distributions/custom_torch_dists.html#CustomDistribution"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scistanpy.model.components.custom_distributions.custom_torch_dists.CustomDistribution" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Base marker class for custom SciStanPy distributions.</p>
<p>This class serves as a marker interface for custom distribution implementations
in SciStanPy. It doesn’t provide any functionality but is useful for type
hinting and identifying custom distributions in the codebase.</p>
<p>All custom distribution classes should inherit from this class to maintain
consistency and enable type checking.</p>
<p><strong>Foundation for Custom Distributions:</strong></p>
<p>The CustomDistribution class provides the PyTorch backend for all custom probability distributions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributions</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>

<span class="c1"># Custom distributions inherit from PyTorch Distribution</span>
<span class="n">custom_dist</span> <span class="o">=</span> <span class="n">MyCustomDistribution</span><span class="p">(</span><span class="n">param1</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">param2</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># Full PyTorch integration</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">custom_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">1000</span><span class="p">,))</span>
<span class="n">log_probs</span> <span class="o">=</span> <span class="n">custom_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Automatic differentiation support</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">custom_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Gradients computed automatically</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>
<section id="log-transformed-distributions">
<h2>Log-Transformed Distributions<a class="headerlink" href="#log-transformed-distributions" title="Link to this heading">¶</a></h2>
<section id="expexponential-distribution">
<h3>ExpExponential Distribution<a class="headerlink" href="#expexponential-distribution" title="Link to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scistanpy.model.components.custom_distributions.custom_torch_dists.ExpExponential">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scistanpy.model.components.custom_distributions.custom_torch_dists.</span></span><span class="sig-name descname"><span class="pre">ExpExponential</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/scistanpy/model/components/custom_distributions/custom_torch_dists.html#ExpExponential"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scistanpy.model.components.custom_distributions.custom_torch_dists.ExpExponential" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code>, <a class="reference internal" href="#scistanpy.model.components.custom_distributions.custom_torch_dists.CustomDistribution" title="scistanpy.model.components.custom_distributions.custom_torch_dists.CustomDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">CustomDistribution</span></code></a></p>
<p>Exponential-Exponential distribution implementation.</p>
<p>This distribution is created by taking the logarithm of an exponentially
distributed random variable. It’s also known as the Gumbel distribution
and is useful for extreme value modeling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rate</strong> (<em>torch.Tensor</em>) – Rate parameter for the underlying exponential distribution</p></li>
<li><p><strong>args</strong> – Additional arguments for the base distribution</p></li>
<li><p><strong>kwargs</strong> – Additional keyword arguments for the base distribution</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Mathematical Definition:</dt><dd><p>If X ~ Exponential(rate), then Y = log(X) ~ ExpExponential(rate)</p>
</dd>
</dl>
<p>Properties:
- Support: (-∞, ∞)
- Type I extreme value distribution (Gumbel)
- Useful for modeling minima of exponential random variables
- Common in survival analysis and reliability engineering</p>
<dl>
<dt>Example:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Extreme value modeling</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rate_param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">exp_exp</span> <span class="o">=</span> <span class="n">ExpExponential</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">rate_param</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">extreme_values</span> <span class="o">=</span> <span class="n">exp_exp</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">1000</span><span class="p">,))</span>
</pre></div>
</div>
</dd>
</dl>
<p>Initialize Exponential-Exponential distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rate</strong> – Rate parameter for base exponential distribution</p></li>
<li><p><strong>args</strong> – Additional arguments</p></li>
<li><p><strong>kwargs</strong> – Additional keyword arguments</p></li>
</ul>
</dd>
</dl>
<p><strong>Log-Exponential Distribution:</strong></p>
<p>If X ~ Exponential(β), then Y = log(X) ~ ExpExponential(β):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Standard exponential vs log-exponential</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># Standard exponential (always positive)</span>
<span class="n">exp_dist</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
<span class="n">exp_samples</span> <span class="o">=</span> <span class="n">exp_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">1000</span><span class="p">,))</span>  <span class="c1"># All positive</span>

<span class="c1"># Log-exponential (can be negative)</span>
<span class="n">log_exp_dist</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">ExpExponential</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
<span class="n">log_samples</span> <span class="o">=</span> <span class="n">log_exp_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">1000</span><span class="p">,))</span>  <span class="c1"># Can be negative</span>

<span class="c1"># Relationship: exp(log_samples) should follow Exponential(beta)</span>
<span class="n">recovered</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_samples</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Mathematical Properties:</strong></p>
<ul class="simple">
<li><p><strong>Support</strong>: Real line (-∞, ∞)</p></li>
<li><p><strong>PDF</strong>: f(y|β) = β exp(y - β exp(y))</p></li>
<li><p><strong>Mean</strong>: -γ - log(β) where γ is Euler’s constant</p></li>
<li><p><strong>Applications</strong>: Log-transformed waiting times, multiplicative processes</p></li>
</ul>
</dd></dl>

</section>
<section id="expdirichlet-distribution">
<h3>ExpDirichlet Distribution<a class="headerlink" href="#expdirichlet-distribution" title="Link to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scistanpy.model.components.custom_distributions.custom_torch_dists.ExpDirichlet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scistanpy.model.components.custom_distributions.custom_torch_dists.</span></span><span class="sig-name descname"><span class="pre">ExpDirichlet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/scistanpy/model/components/custom_distributions/custom_torch_dists.html#ExpDirichlet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scistanpy.model.components.custom_distributions.custom_torch_dists.ExpDirichlet" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code>, <a class="reference internal" href="#scistanpy.model.components.custom_distributions.custom_torch_dists.CustomDistribution" title="scistanpy.model.components.custom_distributions.custom_torch_dists.CustomDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">CustomDistribution</span></code></a></p>
<p>Exponential-Dirichlet distribution implementation.</p>
<p>This distribution is created by taking the element-wise logarithm of a
Dirichlet-distributed random vector. It’s useful for modeling log-scale
compositional data and log-probability vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>concentration</strong> (<em>torch.Tensor</em>) – Concentration parameters for the underlying Dirichlet</p></li>
<li><p><strong>args</strong> – Additional arguments for the base distribution</p></li>
<li><p><strong>kwargs</strong> – Additional keyword arguments for the base distribution</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Mathematical Definition:</dt><dd><p>If X ~ Dirichlet(α), then Y = log(X) ~ ExpDirichlet(α)</p>
</dd>
</dl>
<p>Properties:
- Support: (-∞, 0]^K where K is the number of categories
- Log-scale simplex (sum of exponentials equals 1)
- Natural for log-probability modeling
- Useful in Bayesian analysis of categorical data</p>
<p>This distribution is particularly valuable when working with probability
vectors in log-space, where it maintains the simplex constraint through
the exponential transformation.</p>
<dl>
<dt>Example:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Log-probability vector modeling</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">concentration</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">exp_dirichlet</span> <span class="o">=</span> <span class="n">ExpDirichlet</span><span class="p">(</span><span class="n">concentration</span><span class="o">=</span><span class="n">concentration</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_probs</span> <span class="o">=</span> <span class="n">exp_dirichlet</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">100</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Verify simplex constraint: exp(log_probs).sum(dim=-1) ≈ 1</span>
</pre></div>
</div>
</dd>
</dl>
<p>Initialize Exponential-Dirichlet distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>concentration</strong> – Concentration parameter vector</p></li>
<li><p><strong>args</strong> – Additional arguments</p></li>
<li><p><strong>kwargs</strong> – Additional keyword arguments</p></li>
</ul>
</dd>
</dl>
<p><strong>Log-Simplex Distribution:</strong></p>
<p>If X ~ Dirichlet(α), then Y = log(X) ~ ExpDirichlet(α):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># High-dimensional log-simplex</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>  <span class="c1"># 1000-dimensional simplex</span>

<span class="c1"># Standard Dirichlet (numerical issues with small values)</span>
<span class="n">dirichlet</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
<span class="n">simplex_samples</span> <span class="o">=</span> <span class="n">dirichlet</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">100</span><span class="p">,))</span>

<span class="c1"># Exp-Dirichlet (numerically stable)</span>
<span class="n">exp_dirichlet</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">ExpDirichlet</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
<span class="n">log_simplex_samples</span> <span class="o">=</span> <span class="n">exp_dirichlet</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">100</span><span class="p">,))</span>

<span class="c1"># Verify simplex constraint: exp(log_simplex).sum(dim=-1) ≈ 1</span>
<span class="n">recovered_simplex</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_simplex_samples</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">recovered_simplex</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>
</pre></div>
</div>
<p><strong>Numerical Advantages:</strong></p>
<ul class="simple">
<li><p><strong>Stability</strong>: Avoids underflow with very small probabilities</p></li>
<li><p><strong>High-dimensional</strong>: Handles thousands of categories efficiently</p></li>
<li><p><strong>Constraint</strong>: log(exp(Y).sum()) = 0 (log-simplex constraint)</p></li>
</ul>
</dd></dl>

</section>
<section id="explomax-distribution">
<h3>ExpLomax Distribution<a class="headerlink" href="#explomax-distribution" title="Link to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scistanpy.model.components.custom_distributions.custom_torch_dists.ExpLomax">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scistanpy.model.components.custom_distributions.custom_torch_dists.</span></span><span class="sig-name descname"><span class="pre">ExpLomax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/scistanpy/model/components/custom_distributions/custom_torch_dists.html#ExpLomax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scistanpy.model.components.custom_distributions.custom_torch_dists.ExpLomax" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code>, <a class="reference internal" href="#scistanpy.model.components.custom_distributions.custom_torch_dists.CustomDistribution" title="scistanpy.model.components.custom_distributions.custom_torch_dists.CustomDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">CustomDistribution</span></code></a></p>
<p>Exponential-Lomax distribution implementation.</p>
<p>This distribution is created by taking the logarithm of a Lomax-distributed
random variable. It’s useful for modeling log-scale phenomena that exhibit
heavy-tailed behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lambda</strong> (<em>torch.Tensor</em>) – Scale parameter for the underlying Lomax distribution</p></li>
<li><p><strong>alpha</strong> (<em>torch.Tensor</em>) – Shape parameter for the underlying Lomax distribution</p></li>
<li><p><strong>args</strong> – Additional arguments for the base distribution</p></li>
<li><p><strong>kwargs</strong> – Additional keyword arguments for the base distribution</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Mathematical Definition:</dt><dd><p>If X ~ Lomax(λ, α), then Y = log(X) ~ ExpLomax(λ, α)</p>
</dd>
</dl>
<p>Properties:
- Support: (-∞, ∞)
- Heavy-tailed in log-space
- Useful for log-scale modeling of power-law phenomena
- Natural for multiplicative processes</p>
<p>This distribution combines the heavy-tail properties of the Lomax
distribution with the convenience of log-scale modeling.</p>
<dl>
<dt>Example:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Modeling log-scale heavy-tailed data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lambda_param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">alpha_param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">exp_lomax</span> <span class="o">=</span> <span class="n">ExpLomax</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="n">lambda_param</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha_param</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_samples</span> <span class="o">=</span> <span class="n">exp_lomax</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">1000</span><span class="p">,))</span>
</pre></div>
</div>
</dd>
</dl>
<p>Initialize Exponential-Lomax distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lambda</strong> – Scale parameter</p></li>
<li><p><strong>alpha</strong> – Shape parameter</p></li>
<li><p><strong>args</strong> – Additional arguments</p></li>
<li><p><strong>kwargs</strong> – Additional keyword arguments</p></li>
</ul>
</dd>
</dl>
<p><strong>Log-Heavy-Tailed Distribution:</strong></p>
<p>If X ~ Lomax(λ, α), then Y = log(X) ~ ExpLomax(λ, α):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Heavy-tailed phenomena on log scale</span>
<span class="n">lambda_param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>  <span class="c1"># Shape parameter</span>

<span class="c1"># Standard Lomax (heavy right tail)</span>
<span class="n">lomax</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">Lomax</span><span class="p">(</span><span class="n">lambda_param</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
<span class="n">lomax_samples</span> <span class="o">=</span> <span class="n">lomax</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">1000</span><span class="p">,))</span>

<span class="c1"># Log-Lomax (heavy tails on log scale)</span>
<span class="n">exp_lomax</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">ExpLomax</span><span class="p">(</span><span class="n">lambda_param</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
<span class="n">log_samples</span> <span class="o">=</span> <span class="n">exp_lomax</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">1000</span><span class="p">,))</span>

<span class="c1"># Applications: Log-income, log-wealth, log-city-sizes</span>
<span class="n">log_wealth</span> <span class="o">=</span> <span class="n">exp_lomax</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">10000</span><span class="p">,))</span>
</pre></div>
</div>
<p><strong>Applications:</strong></p>
<ul class="simple">
<li><p><strong>Economics</strong>: Log-wealth, log-income distributions</p></li>
<li><p><strong>Urban studies</strong>: Log-city sizes (Zipf’s law)</p></li>
<li><p><strong>Internet</strong>: Log-file sizes, log-connection times</p></li>
</ul>
</dd></dl>

</section>
</section>
<section id="heavy-tailed-distributions">
<h2>Heavy-Tailed Distributions<a class="headerlink" href="#heavy-tailed-distributions" title="Link to this heading">¶</a></h2>
<section id="lomax-distribution">
<h3>Lomax Distribution<a class="headerlink" href="#lomax-distribution" title="Link to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scistanpy.model.components.custom_distributions.custom_torch_dists.Lomax">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scistanpy.model.components.custom_distributions.custom_torch_dists.</span></span><span class="sig-name descname"><span class="pre">Lomax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/scistanpy/model/components/custom_distributions/custom_torch_dists.html#Lomax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scistanpy.model.components.custom_distributions.custom_torch_dists.Lomax" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code>, <a class="reference internal" href="#scistanpy.model.components.custom_distributions.custom_torch_dists.CustomDistribution" title="scistanpy.model.components.custom_distributions.custom_torch_dists.CustomDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">CustomDistribution</span></code></a></p>
<p>Lomax distribution implementation (shifted Pareto distribution).</p>
<p>The Lomax distribution is a shifted version of the Pareto distribution,
also known as the Pareto Type II distribution. It’s implemented as a
transformed Pareto distribution with an affine transformation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lambda</strong> (<em>torch.Tensor</em>) – Scale parameter (must be positive)</p></li>
<li><p><strong>alpha</strong> (<em>torch.Tensor</em>) – Shape parameter (must be positive)</p></li>
<li><p><strong>args</strong> – Additional arguments for the base distribution</p></li>
<li><p><strong>kwargs</strong> – Additional keyword arguments for the base distribution</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Mathematical Definition:</dt><dd><p>If X ~ Pareto(scale=λ, shape=α), then Y = X - λ ~ Lomax(λ, α)</p>
</dd>
</dl>
<p>Properties:
- Support: [0, ∞)
- Heavy-tailed distribution
- Power-law behavior in the tail</p>
<p>The distribution is implemented using PyTorch’s TransformedDistribution
framework with a Pareto base distribution and an affine transformation.</p>
<dl>
<dt>Example:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Lomax distribution for modeling heavy-tailed phenomena</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lambda_param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">alpha_param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lomax</span> <span class="o">=</span> <span class="n">Lomax</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="n">lambda_param</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha_param</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">samples</span> <span class="o">=</span> <span class="n">lomax</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">1000</span><span class="p">,))</span>
</pre></div>
</div>
</dd>
</dl>
<p>Initialize Lomax distribution as transformed Pareto.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lambda</strong> – Scale parameter</p></li>
<li><p><strong>alpha</strong> – Shape parameter</p></li>
<li><p><strong>args</strong> – Additional base distribution arguments</p></li>
<li><p><strong>kwargs</strong> – Additional base distribution keyword arguments</p></li>
</ul>
</dd>
</dl>
<p><strong>Power-Law Tail Distribution:</strong></p>
<p>The Lomax distribution (Pareto Type II) has polynomial tails:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Insurance claims with heavy tails</span>
<span class="n">lambda_param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1000.0</span><span class="p">)</span>  <span class="c1"># Scale parameter</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.2</span><span class="p">)</span>            <span class="c1"># Shape (tail index)</span>

<span class="n">lomax_dist</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">Lomax</span><span class="p">(</span><span class="n">lambda_param</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

<span class="c1"># Generate insurance claims</span>
<span class="n">claims</span> <span class="o">=</span> <span class="n">lomax_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">10000</span><span class="p">,))</span>

<span class="c1"># Heavy tail: P(X &gt; x) ∼ x^(-α) for large x</span>
<span class="c1"># Lower α → heavier tails</span>
<span class="n">very_heavy</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">Lomax</span><span class="p">(</span><span class="n">lambda_param</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>  <span class="c1"># Very heavy tails</span>
<span class="n">moderate</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">Lomax</span><span class="p">(</span><span class="n">lambda_param</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>    <span class="c1"># Moderate tails</span>
</pre></div>
</div>
<p><strong>Tail Behavior:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Tail probability comparison</span>
<span class="n">x_large</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span>

<span class="c1"># Lomax: P(X &gt; x) ∼ (λ/(λ+x))^α</span>
<span class="n">lomax_tail</span> <span class="o">=</span> <span class="n">lomax_dist</span><span class="o">.</span><span class="n">ccdf</span><span class="p">(</span><span class="n">x_large</span><span class="p">)</span>

<span class="c1"># Compare to exponential tail: P(X &gt; x) ∼ exp(-x/β)</span>
<span class="n">exp_tail</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mf">1000.0</span><span class="p">)</span><span class="o">.</span><span class="n">ccdf</span><span class="p">(</span><span class="n">x_large</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Lomax tail probability: </span><span class="si">{</span><span class="n">lomax_tail</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exponential tail probability: </span><span class="si">{</span><span class="n">exp_tail</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Lomax will be much larger for extreme values</span>
</pre></div>
</div>
</dd></dl>

<p><strong>Mathematical Properties:</strong></p>
<ul class="simple">
<li><p><strong>Support</strong>: [0, ∞)</p></li>
<li><p><strong>PDF</strong>: f(x|λ,α) = α λ^α / (λ + x)^(α+1)</p></li>
<li><p><strong>Mean</strong>: λ/(α-1) for α &gt; 1</p></li>
<li><p><strong>Variance</strong>: λ² α / ((α-1)² (α-2)) for α &gt; 2</p></li>
<li><p><strong>Tail index</strong>: α (lower values = heavier tails)</p></li>
</ul>
</section>
</section>
<section id="multinomial-variants">
<h2>Multinomial Variants<a class="headerlink" href="#multinomial-variants" title="Link to this heading">¶</a></h2>
<section id="multinomiallogit-distribution">
<h3>MultinomialLogit Distribution<a class="headerlink" href="#multinomiallogit-distribution" title="Link to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scistanpy.model.components.custom_distributions.custom_torch_dists.MultinomialLogit">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scistanpy.model.components.custom_distributions.custom_torch_dists.</span></span><span class="sig-name descname"><span class="pre">MultinomialLogit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">total_count</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'custom_types.Integer'</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/scistanpy/model/components/custom_distributions/custom_torch_dists.html#MultinomialLogit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scistanpy.model.components.custom_distributions.custom_torch_dists.MultinomialLogit" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Multinomial</span></code>, <a class="reference internal" href="#scistanpy.model.components.custom_distributions.custom_torch_dists.CustomDistribution" title="scistanpy.model.components.custom_distributions.custom_torch_dists.CustomDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">CustomDistribution</span></code></a></p>
<p>Multinomial distribution parameterized by logits.</p>
<p>This class provides a specialized interface for multinomial distributions
where the parameters are specified as logits (log-odds) rather than
probabilities. This parameterization is often more convenient for
modeling and optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>total_count</strong> (<em>Union</em><em>[</em><em>custom_types.Integer</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Total number of trials for each batch element. Defaults to 1.</p></li>
<li><p><strong>logits</strong> (<em>Optional</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Event logits (log-odds)</p></li>
<li><p><strong>validate_args</strong> (<em>Optional</em><em>[</em><em>bool</em><em>]</em>) – Whether to validate arguments. Defaults to None.</p></li>
</ul>
</dd>
</dl>
<p>The logit parameterization is advantageous because:
- No normalization constraints (logits can be any real numbers)
- Better numerical properties for optimization
- Natural output of neural networks and linear models
- Automatic normalization through softmax transformation</p>
<dl>
<dt>Example:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Logit parameterization</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># No normalization needed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">total_counts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">50</span><span class="p">],</span> <span class="p">[</span><span class="mi">75</span><span class="p">],</span> <span class="p">[</span><span class="mi">100</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span> <span class="o">=</span> <span class="n">MultinomialLogit</span><span class="p">(</span><span class="n">total_count</span><span class="o">=</span><span class="n">total_counts</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<p>Initialize logit-parameterized multinomial distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>total_count</strong> – Total trials per batch element</p></li>
<li><p><strong>logits</strong> – Logit parameters (must be provided)</p></li>
<li><p><strong>validate_args</strong> – Validation flag</p></li>
</ul>
</dd>
</dl>
<p><strong>Unconstrained Multinomial Parameterization:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Unconstrained parameterization for optimization</span>
<span class="n">n_categories</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">total_trials</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Logits can be any real values</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_categories</span><span class="p">)</span>  <span class="c1"># Unconstrained</span>

<span class="c1"># Multinomial with logit parameterization</span>
<span class="n">multinomial_logit</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">MultinomialLogit</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">total_trials</span><span class="p">)</span>

<span class="c1"># Sample category counts</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">multinomial_logit</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="n">total_trials</span>

<span class="c1"># Convert to probabilities</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Optimization Advantages:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Optimization-friendly parameterization</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MulticlassModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">):</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ssp</span><span class="o">.</span><span class="n">MultinomialLogit</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">n_trials</span><span class="p">)</span>

<span class="c1"># No constraints on logits during optimization</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MulticlassModel</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="multinomiallogtheta-distribution">
<h3>MultinomialLogTheta Distribution<a class="headerlink" href="#multinomiallogtheta-distribution" title="Link to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="scistanpy.model.components.custom_distributions.custom_torch_dists.MultinomialLogTheta">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">scistanpy.model.components.custom_distributions.custom_torch_dists.</span></span><span class="sig-name descname"><span class="pre">MultinomialLogTheta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">total_count</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'custom_types.Integer'</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_probs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validate_args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/scistanpy/model/components/custom_distributions/custom_torch_dists.html#MultinomialLogTheta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#scistanpy.model.components.custom_distributions.custom_torch_dists.MultinomialLogTheta" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#scistanpy.model.components.custom_distributions.custom_torch_dists.MultinomialLogit" title="scistanpy.model.components.custom_distributions.custom_torch_dists.MultinomialLogit"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultinomialLogit</span></code></a></p>
<p>Multinomial distribution with normalized log-probabilities.</p>
<p>This class extends MultinomialLogit with the additional constraint that
the input log-probabilities must already be normalized (i.e., their
exponentials sum to 1). This is useful when working with log-probability
vectors that are guaranteed to be valid probability distributions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>total_count</strong> (<em>Union</em><em>[</em><em>custom_types.Integer</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Total number of trials for each batch element. Defaults to 1.</p></li>
<li><p><strong>log_probs</strong> (<em>Optional</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Normalized log-probabilities (exp(log_probs) must sum to 1)</p></li>
<li><p><strong>validate_args</strong> (<em>Optional</em><em>[</em><em>bool</em><em>]</em>) – Whether to validate arguments. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>AssertionError</strong> – If log_probs is None</p></li>
<li><p><strong>AssertionError</strong> – If log_probs are not properly normalized</p></li>
</ul>
</dd>
</dl>
<p>This parameterization is particularly useful when:
- Working with log-space normalized probability vectors
- Ensuring numerical precision in log-space computations
- Interfacing with other log-space probability calculations</p>
<p>The normalization constraint is enforced at initialization to prevent
invalid probability distributions.</p>
<dl>
<dt>Example:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Normalized log-probabilities</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">total_counts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">100</span><span class="p">],</span> <span class="p">[</span><span class="mi">200</span><span class="p">],</span> <span class="p">[</span><span class="mi">150</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span> <span class="o">=</span> <span class="n">MultinomialLogTheta</span><span class="p">(</span><span class="n">total_count</span><span class="o">=</span><span class="n">total_counts</span><span class="p">,</span> <span class="n">log_probs</span><span class="o">=</span><span class="n">log_probs</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<p>Initialize normalized log-probability multinomial distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>total_count</strong> – Total trials per batch element</p></li>
<li><p><strong>log_probs</strong> – Normalized log-probability parameters</p></li>
<li><p><strong>validate_args</strong> – Validation flag</p></li>
</ul>
</dd>
</dl>
<p>Validates that log_probs are properly normalized before initialization.</p>
<p><strong>Log-Probability Parameterized with Optimization:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Log-simplex parameterization</span>
<span class="n">log_probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]))</span>
<span class="n">total_trials</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Efficient for large N with automatic coefficient handling</span>
<span class="n">multinomial_log</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">MultinomialLogTheta</span><span class="p">(</span>
    <span class="n">log_theta</span><span class="o">=</span><span class="n">log_probabilities</span><span class="p">,</span>
    <span class="n">N</span><span class="o">=</span><span class="n">total_trials</span>
<span class="p">)</span>

<span class="c1"># For observed data, coefficient is precomputed</span>
<span class="n">observed_counts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">180</span><span class="p">,</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">95</span><span class="p">,</span> <span class="mi">405</span><span class="p">])</span>
<span class="n">log_prob</span> <span class="o">=</span> <span class="n">multinomial_log</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">observed_counts</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Stan Integration Benefits:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># When used as observable in SciStanPy models</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">MultinomialLogTheta</span><span class="p">(</span>
    <span class="n">log_theta</span><span class="o">=</span><span class="n">log_probs</span><span class="p">,</span>
    <span class="n">N</span><span class="o">=</span><span class="n">total_n</span><span class="p">,</span>
    <span class="n">observable</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># Triggers Stan optimization</span>
<span class="p">)</span>

<span class="c1"># Generated Stan code precomputes multinomial coefficient</span>
<span class="c1"># in transformed data block for efficiency</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>
<section id="pytorch-integration-features">
<h2>PyTorch Integration Features<a class="headerlink" href="#pytorch-integration-features" title="Link to this heading">¶</a></h2>
<p><strong>Automatic Differentiation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gradients flow through all custom distributions</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">custom_dist</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">ExpExponential</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Likelihood with gradients</span>
<span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">custom_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">log_likelihood</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient w.r.t. beta: </span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Vectorization Support:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Batch processing with vectorized operations</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">n_params</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Vectorized parameters</span>
<span class="n">betas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_params</span><span class="p">)</span>

<span class="c1"># Vectorized distribution</span>
<span class="n">vectorized_dist</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">ExpExponential</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="n">betas</span><span class="p">)</span>

<span class="c1"># Vectorized sampling and evaluation</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">vectorized_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Shape: (batch_size, n_params)</span>
<span class="n">log_probs</span> <span class="o">=</span> <span class="n">vectorized_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>GPU Acceleration:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># GPU computation for large-scale problems</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

<span class="c1"># Move parameters to GPU</span>
<span class="n">beta_gpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># GPU-accelerated distribution</span>
<span class="n">gpu_dist</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">ExpExponential</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="n">beta_gpu</span><span class="p">)</span>

<span class="c1"># GPU sampling and computation</span>
<span class="n">gpu_samples</span> <span class="o">=</span> <span class="n">gpu_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">100000</span><span class="p">,))</span>  <span class="c1"># Fast GPU sampling</span>
<span class="n">gpu_log_probs</span> <span class="o">=</span> <span class="n">gpu_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">gpu_samples</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="numerical-stability-features">
<h2>Numerical Stability Features<a class="headerlink" href="#numerical-stability-features" title="Link to this heading">¶</a></h2>
<p><strong>Log-Space Arithmetic:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Numerically stable implementations</span>

<span class="c1"># ExpDirichlet uses log-sum-exp for stability</span>
<span class="n">large_alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1e6</span><span class="p">,</span> <span class="mf">1e6</span><span class="p">,</span> <span class="mf">1e6</span><span class="p">])</span>
<span class="n">stable_exp_dir</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">ExpDirichlet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">large_alpha</span><span class="p">)</span>

<span class="c1"># ExpLomax handles extreme scale parameters</span>
<span class="n">huge_lambda</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e10</span><span class="p">)</span>
<span class="n">stable_exp_lomax</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">ExpLomax</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="n">huge_lambda</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Gradient Stability:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Stable gradients even with extreme parameters</span>
<span class="n">extreme_beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">exp_exp_dist</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">ExpExponential</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="n">extreme_beta</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">])</span>

<span class="n">log_prob</span> <span class="o">=</span> <span class="n">exp_exp_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">log_prob</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Stable gradients</span>
</pre></div>
</div>
</section>
<section id="performance-optimization">
<h2>Performance Optimization<a class="headerlink" href="#performance-optimization" title="Link to this heading">¶</a></h2>
<p><strong>Efficient Sampling:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Optimized sampling algorithms</span>
<span class="n">dist</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">ExpExponential</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Uses inverse transform sampling for efficiency</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">10000</span><span class="p">,))</span>
</pre></div>
</div>
<p><strong>Memory-Efficient Operations:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># In-place operations where possible</span>
<span class="n">large_dist</span> <span class="o">=</span> <span class="n">ssp</span><span class="o">.</span><span class="n">Lomax</span><span class="p">(</span><span class="n">lambda_</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="c1"># Memory-efficient evaluation</span>
<span class="n">large_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span>
<span class="n">log_probs</span> <span class="o">=</span> <span class="n">large_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">large_data</span><span class="p">)</span>  <span class="c1"># Efficient computation</span>
</pre></div>
</div>
</section>
<section id="best-practices">
<h2>Best Practices<a class="headerlink" href="#best-practices" title="Link to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p><strong>Use appropriate distributions</strong> for your data characteristics</p></li>
<li><p><strong>Leverage log-space parameterizations</strong> for numerical stability</p></li>
<li><p><strong>Take advantage of vectorization</strong> for batch processing</p></li>
<li><p><strong>Monitor gradient flow</strong> in optimization problems</p></li>
<li><p><strong>Use GPU acceleration</strong> for large-scale computations</p></li>
<li><p><strong>Test numerical stability</strong> with extreme parameter values</p></li>
<li><p><strong>Profile performance</strong> for computational bottlenecks</p></li>
</ol>
<p>The PyTorch custom distributions provide the computational foundation for advanced probabilistic modeling while maintaining full integration with PyTorch’s automatic differentiation and GPU acceleration capabilities.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index.html">SciStanPy</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../index.html">SciStanPy API Reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#scope">Scope</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#top-level-package">Top-Level Package</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#module-overview">Module Overview</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../../index.html#quick-navigation">Quick Navigation</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../../custom_types.html">Custom Types API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../defaults.html">Defaults API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../exceptions.html">Exceptions API Reference</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../../index.html">Model API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../plotting/index.html">Plotting API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../operations.html">Operations API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../utils.html">Utils API Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#usage-notes">Usage Notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#stability">Stability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#missing-something">Missing Something?</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index.html">Documentation overview</a><ul>
  <li><a href="../../../index.html">SciStanPy API Reference</a><ul>
  <li><a href="../../index.html">Model API Reference</a><ul>
  <li><a href="../index.html">Model Components API Reference</a><ul>
  <li><a href="index.html">Custom Distributions API Reference</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">Custom Distributions API Reference</a></li>
      <li>Next: <a href="custom_scipy_dists.html" title="next chapter">Custom SciPy Distributions API Reference</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Bruce Wittmann.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.3.0</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../../../../_sources/api/model/components/custom_distributions/custom_torch_dists.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>